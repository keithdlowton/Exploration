{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0f41512-1c57-4dea-92ce-c60b6102e39f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import regex as re\n",
    "\n",
    "from prophet import Prophet\n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "#from learntools.time_series.utils import plot_periodogram, seasonal_plot\n",
    "\n",
    "import sktime\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b81cee-fc70-43e6-9d95-58021f74633e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f160aaeb-c902-4b61-bcea-7c1f78aea017",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "### Functions ###\n",
    "#################\n",
    "\n",
    "def get_color(name, number):\n",
    "    pal = list(sns.color_palette(palette=name, n_colors=number).as_hex())\n",
    "    return pal\n",
    "\n",
    "def add_datepart(df, fldname, drop=True, time=False):\n",
    "    \"Helper function that adds columns relevant to a date.\"\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "    \n",
    "def holiday(name):\n",
    "    dset = holidays_events[holidays_events.locale == name]\n",
    "    dset = dset[['date','var']].reset_index(drop=True)\n",
    "    dset.rename(columns = {'var':name}, inplace = True)\n",
    "    dset = dset.drop_duplicates()\n",
    "    dset.date = dset.date.astype('datetime64[ns]')\n",
    "    return dset    \n",
    "  \n",
    "def train_test_plot(train,test,name):\n",
    "\n",
    "    y_train = train.y\n",
    "    y_test = test.y\n",
    "    y_test.index = y_test.index + max(y_train.index)\n",
    "    print(y_train.shape[0], y_test.shape[0])\n",
    "    \n",
    "    f = plt.figure(figsize=(19, 15))\n",
    "    plot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"], title = f'{name} Train-test plot');\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "    plot_series(y_test, title = f'{name} Test plot');\n",
    "    plt.show    \n",
    "\n",
    "########################\n",
    "### Cross validation ###\n",
    "########################\n",
    "\n",
    "def cv_exam(model, test_df, y_test, initial, period, horizon):\n",
    "    \n",
    "    forecast = model.predict(test_df)\n",
    "    df_cv = cross_validation(model, initial = initial, period = period, horizon = horizon)\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    df_cv['residual'] = df_cv['y'] - df_cv['yhat']\n",
    "    prop_test = pd.merge(test_df,forecast[['yhat','ds']],on=['ds'],how='inner')\n",
    "    prop_test.index = y_test.index\n",
    "    \n",
    "    return df_cv, df_p, forecast, prop_test\n",
    "\n",
    "def perf_vals(dset, meas = 'smape'):\n",
    "    rmse_add = dset.groupby(['Model'])[meas].mean()\n",
    "    rmse_add = pd.DataFrame(rmse_add)\n",
    "    a = rmse_add.sort_values(by = [meas])\n",
    "    \n",
    "    rmse_add = rmse_add.reset_index()\n",
    "    model = rmse_add[rmse_add[meas] == min(rmse_add[meas])]\n",
    "    print(f\"The best model is the {model['Model'].values[0]} model\")\n",
    "\n",
    "    return a\n",
    "\n",
    "def mod_diag(model, mod_name, test_df, y_train, y_test, name, initial, period, horizon):\n",
    "    f = plt.figure(figsize=(19, 15))\n",
    "    plot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"], title = f'{name} Train-test plot');\n",
    "    \n",
    "    df_cv, df_p, forecast, prop_test = cv_exam(model, test_df, y_test, initial, period, horizon)\n",
    "    fig = model.plot(forecast)\n",
    "    a = add_changepoints_to_plot(fig.gca(), model, forecast)\n",
    "    mape_ph = mean_absolute_percentage_error(prop_test['y'], prop_test['yhat'], symmetric=True)\n",
    "    mse_ph = mean_squared_error(prop_test['y'], prop_test['yhat'], squared=False)\n",
    "    print(f\"The Smape loss value for {name} and {mod_name} model is {mape_ph:.6f}\")\n",
    "    print(f\"The RMSE loss value for {name} and {mod_name} model is {mse_ph:.6f}\")\n",
    "\n",
    "    plot_series(prop_test['yhat'], y_test, labels=[\"y_pred\",\"y_test\"], title = f'{name} {mod_name} Test predict-actuals plot')\n",
    "    plt.show();\n",
    "    \n",
    "    fig = model.plot_components(forecast)\n",
    "    f = plt.figure(figsize=(19, 15))\n",
    "    ax = sns.lineplot(x=\"ds\", y=\"residual\", markers=True, data=df_cv)\n",
    "    ax.set(xlabel='Dates', ylabel='Residuals')\n",
    "    plt.show();\n",
    "    return df_cv, df_p, forecast, prop_test\n",
    "\n",
    "def perf_vals(dset, meas = 'smape'):\n",
    "    rmse_add = dset.groupby(['Model'])[meas].mean()\n",
    "    rmse_add = pd.DataFrame(rmse_add)\n",
    "    a = rmse_add.sort_values(by = [meas])\n",
    "    \n",
    "    rmse_add = rmse_add.reset_index()\n",
    "    model = rmse_add[rmse_add[meas] == min(rmse_add[meas])]\n",
    "    print(f\"The best model is the {model['Model'].values[0]} model\")\n",
    "\n",
    "    return a\n",
    "\n",
    "def res_data(dset,meas,name,res_dset):\n",
    "    try:\n",
    "        baby = perf_vals(dset,meas)        \n",
    "        baby[str(name + ' Model')] = baby.index\n",
    "        baby.rename(columns = {meas : str(name) + ' ' + str(meas.capitalize())}, inplace = True)\n",
    "        baby = baby.reset_index(drop=True)\n",
    "        res_dset = pd.concat([res_dset,baby], axis = 1)\n",
    "        res_dset = res_dset.reset_index(drop=True)\n",
    "        return res_dset\n",
    "    except:\n",
    "        print('No ' + name + ' data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ed3c911-e547-489e-98b3-08929d800527",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_dset(dset,name,tt):\n",
    "    dset = dset.drop(['month','year','day_of_week','day_of_year','sales','family'],axis = 1)\n",
    "    dset.date = dset.date.astype('datetime64[ns]')\n",
    "    time_data_ph = pd.merge(tt[tt.family == name],dset,on = 'date',how = 'left')\n",
    "    time_data_ph.rename(columns = {'sales':'y', 'date':'ds'}, inplace = True)\n",
    "\n",
    "    time_data_ph = time_data_ph.sort_values('ds')\n",
    "    time_data_ph['y'] = time_data_ph['y'].astype(int)\n",
    "    time_data_ph['ds'] = pd.to_datetime(time_data_ph['ds'], format='%Y-%m-%d')\n",
    "    time_data_ph = time_data_ph.reset_index(drop=True)\n",
    "\n",
    "    for name in ['National','Regional','Local','Is_month_end','Is_year_start','Is_month_end','Is_month_start','Is_month_end','Is_quarter_start','Is_year_end','Is_quarter_end']:\n",
    "        time_data_ph[name] = time_data_ph[name].astype(int)\n",
    "    return time_data_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "add5e61a-4e8d-4670-8506-3cada0968c32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### Model framework ###\n",
    "#######################\n",
    "\n",
    "def cap_floor(train_dset, test_dset, name, initial, period, horizon, growth):\n",
    "    \n",
    "    train_dset['cap'] = max(int(max(train_dset.y) * 1.1),int(max(test_dset.y) * 1.1))\n",
    "    train_dset['floor'] = 0\n",
    "    test_dset['cap'] = train_dset['cap']\n",
    "    test_dset['floor'] = 0\n",
    "    #return train_df, test_dset\n",
    "\n",
    "    ### Baseline model ###\n",
    "\n",
    "    m = Prophet(growth = growth)\n",
    "    m.fit(train_dset)\n",
    "\n",
    "    ### Strict model ###\n",
    "\n",
    "    m_strict = Prophet(changepoint_prior_scale=0.01,\n",
    "                       weekly_seasonality=True, \n",
    "                       daily_seasonality=False, \n",
    "                       yearly_seasonality=True,\n",
    "                       seasonality_mode = 'multiplicative',\n",
    "                       growth = growth)\n",
    "    m_strict.fit(train_dset)\n",
    "\n",
    "    ### Flexible model ###\n",
    "\n",
    "    m_flex = Prophet(changepoint_prior_scale=0.5, growth = growth)\n",
    "    m_flex.fit(train_dset)\n",
    "\n",
    "    ### Holiday model ###\n",
    "\n",
    "    m_hol = Prophet(holidays=holidays, growth = growth)\n",
    "    m_hol.add_regressor(train_dset.filter(regex='dcoilwtico').columns[0])\n",
    "    m_hol.add_regressor('Is_month_end')\n",
    "    m_hol.add_regressor('Year')\n",
    "    m_hol.add_regressor('Month')\n",
    "    m_hol.add_regressor('Week')\n",
    "    m_hol.add_regressor('Dayofweek')\n",
    "    m_hol.add_regressor('Dayofyear')\n",
    "    m_hol.add_regressor('Is_month_start')\n",
    "    m_hol.add_regressor('Is_quarter_end')\n",
    "    m_hol.add_regressor('Is_quarter_start')\n",
    "    m_hol.add_regressor('Is_year_end')\n",
    "    m_hol.add_regressor('Is_year_start')\n",
    "    m_hol.add_regressor(train_dset.filter(regex='is_holiday').columns[0])\n",
    "    m_hol.add_regressor(train_dset.filter(regex='onpromotion').columns[0])\n",
    "    m_hol.fit(train_dset)\n",
    "\n",
    "    ### Regressors ###\n",
    "\n",
    "    m_reg = Prophet(changepoint_prior_scale=0.01, growth = growth)\n",
    "    m_reg.add_regressor(train_dset.filter(regex='dcoilwtico').columns[0])\n",
    "    m_reg.add_regressor(train_dset.filter(regex='is_holiday').columns[0])\n",
    "    m_reg.add_regressor(train_dset.filter(regex='onpromotion').columns[0])\n",
    "    m_reg.add_regressor('National')\n",
    "    m_reg.add_regressor('Regional')\n",
    "    m_reg.add_regressor('Local')    \n",
    "    m_reg.add_regressor('Is_month_end')\n",
    "    m_reg.add_regressor('Year')\n",
    "    m_reg.add_regressor('Month')\n",
    "    m_reg.add_regressor('Week')\n",
    "    m_reg.add_regressor('Dayofweek')\n",
    "    m_reg.add_regressor('Dayofyear')\n",
    "    m_reg.add_regressor('Is_month_start')\n",
    "    m_reg.add_regressor('Is_quarter_end')\n",
    "    m_reg.add_regressor('Is_quarter_start')\n",
    "    m_reg.add_regressor('Is_year_end')\n",
    "    m_reg.add_regressor('Is_year_start')\n",
    "    m_reg.fit(train_dset)\n",
    "\n",
    "    ### Holiday-Regressors-Seasonality ###\n",
    "\n",
    "    m_hrs = Prophet(weekly_seasonality=True, \n",
    "                    daily_seasonality=False, \n",
    "                    yearly_seasonality=True,\n",
    "                    seasonality_mode = 'multiplicative',\n",
    "                    changepoint_prior_scale = 0.1, \n",
    "                    seasonality_prior_scale = 10.0,\n",
    "                    holidays=holidays,\n",
    "                    growth = growth)\n",
    "    m_hrs.add_regressor(train_dset.filter(regex='dcoilwtico').columns[0])\n",
    "    m_hrs.add_regressor(train_dset.filter(regex='onpromotion').columns[0])\n",
    "    m_hrs.add_regressor(train_dset.filter(regex='is_holiday').columns[0])\n",
    "    m_hrs.add_regressor('National')\n",
    "    m_hrs.add_regressor('Regional')\n",
    "    m_hrs.add_regressor('Local')    \n",
    "    m_hrs.add_regressor('Is_month_end')\n",
    "    m_hrs.add_regressor('Year')\n",
    "    m_hrs.add_regressor('Month')\n",
    "    m_hrs.add_regressor('Week')\n",
    "    m_hrs.add_regressor('Dayofweek')\n",
    "    m_hrs.add_regressor('Dayofyear')\n",
    "    m_hrs.add_regressor('Is_month_start')\n",
    "    m_hrs.add_regressor('Is_quarter_end')\n",
    "    m_hrs.add_regressor('Is_quarter_start')\n",
    "    m_hrs.add_regressor('Is_year_end')\n",
    "    m_hrs.add_regressor('Is_year_start')\n",
    "    m_hrs.fit(train_dset)\n",
    "\n",
    "    ### Holiday-Regressors-Seasonality ###\n",
    "\n",
    "    m_comp = Prophet(growth = growth,\n",
    "                    seasonality_mode = 'additive',      # seasonality_mode = 'multiplicative',\n",
    "                    changepoint_prior_scale = 0.1, \n",
    "                    seasonality_prior_scale = 10.0,\n",
    "                    holidays_prior_scale = 20.0,\n",
    "                    weekly_seasonality = False, \n",
    "                    daily_seasonality = False, \n",
    "                    yearly_seasonality = False,\n",
    "                    holidays=holidays,\n",
    "                    ).add_seasonality(\n",
    "                        name = 'monthly',\n",
    "                        period = 30.5,\n",
    "                        fourier_order = 55\n",
    "                    #).add_seasonality(\n",
    "                        #name = 'daily',\n",
    "                        #period = 1,\n",
    "                        #fourier_order = 3,\n",
    "                        #prior_scale = 30\n",
    "                    ).add_seasonality(\n",
    "                        name = 'weekly',\n",
    "                        period = 7,\n",
    "                        fourier_order = 10,\n",
    "                        prior_scale = 40\n",
    "                    ).add_seasonality(\n",
    "                        name = 'yearly',\n",
    "                        period = 365.25,\n",
    "                        fourier_order = 20\n",
    "                    ).add_seasonality(\n",
    "                        name = 'quarterly',\n",
    "                        period = 365.25 / 4,\n",
    "                        fourier_order = 5,\n",
    "                        prior_scale = 15\n",
    "                    ).add_seasonality(\n",
    "                        name = 'bi-monthly',\n",
    "                        period = 365.25 / 6,\n",
    "                        fourier_order = 5,\n",
    "                        prior_scale = 15\n",
    "                    )\n",
    "\n",
    "    m_comp.add_regressor(train_dset.filter(regex='dcoilwtico').columns[0])\n",
    "    m_comp.add_regressor(train_dset.filter(regex='onpromotion').columns[0])\n",
    "    m_comp.add_regressor(train_dset.filter(regex='is_holiday').columns[0])\n",
    "    m_comp.add_regressor('National')\n",
    "    m_comp.add_regressor('Regional')\n",
    "    m_comp.add_regressor('Local')    \n",
    "    m_comp.add_regressor('Is_month_end')\n",
    "    m_comp.add_regressor('Year')\n",
    "    m_comp.add_regressor('Month')\n",
    "    m_comp.add_regressor('Week')\n",
    "    m_comp.add_regressor('Dayofweek')\n",
    "    m_comp.add_regressor('Dayofyear')\n",
    "    m_comp.add_regressor('Is_month_start')\n",
    "    m_comp.add_regressor('Is_quarter_end')\n",
    "    m_comp.add_regressor('Is_quarter_start')\n",
    "    m_comp.add_regressor('Is_year_end')\n",
    "    m_comp.add_regressor('Is_year_start')\n",
    "    m_comp.fit(train_dset)\n",
    "\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    y_train = train_dset.y\n",
    "    y_test = test_dset.y\n",
    "    y_test.index = y_test.index + max(y_train.index)\n",
    "    print(y_train.shape[0], y_test.shape[0])\n",
    "    \n",
    "    df_cv, df_p, forecast, prop_test = mod_diag(m, 'Baseline model', test_dset, y_train, y_test, name, initial, period, horizon)\n",
    "    df_p['Model'] = 'Baseline'\n",
    "    df_all = pd.concat([df_all,df_p], axis = 0).reset_index(drop=True)\n",
    "    print(df_all.shape)\n",
    "    \n",
    "    df_cv, df_p, forecast, prop_test = mod_diag(m_strict, 'Strict model', test_dset, y_train, y_test, name, initial, period, horizon)\n",
    "    df_p['Model'] = 'Strict'\n",
    "    df_all = pd.concat([df_all,df_p], axis = 0).reset_index(drop=True)\n",
    "    print(df_all.shape)\n",
    "    \n",
    "    df_cv, df_p, forecast, prop_test = mod_diag(m_flex, 'Flexible model', test_dset, y_train, y_test, name, initial, period, horizon)\n",
    "    df_p['Model'] = 'Flexible'\n",
    "    df_all = pd.concat([df_all,df_p], axis = 0).reset_index(drop=True)\n",
    "    print(df_all.shape)\n",
    "    \n",
    "    df_cv, df_p, forecast, prop_test = mod_diag(m_reg, 'Regressor model', test_dset, y_train, y_test, name, initial, period, horizon)\n",
    "    df_p['Model'] = 'Regressor'\n",
    "    df_all = pd.concat([df_all,df_p], axis = 0).reset_index(drop=True)\n",
    "    print(df_all.shape)\n",
    "    \n",
    "    df_cv, df_p, forecast, prop_test = mod_diag(m_hol, 'Holiday model', test_dset, y_train, y_test, name, initial, period, horizon)\n",
    "    df_p['Model'] = 'Holiday'\n",
    "    df_all = pd.concat([df_all,df_p], axis = 0).reset_index(drop=True)\n",
    "    print(df_all.shape)\n",
    "    \n",
    "    df_cv, df_p, forecast, prop_test = mod_diag(m_hrs, 'Holiday-Regressor-Seasonal model', test_dset, y_train, y_test, name, initial, period, horizon)\n",
    "    df_p['Model'] = 'HRS'\n",
    "    df_all = pd.concat([df_all,df_p], axis = 0).reset_index(drop=True)\n",
    "    print(df_all.shape)\n",
    "    \n",
    "    df_cv, df_p, forecast, prop_test = mod_diag(m_comp, 'Constructed model', test_dset, y_train, y_test, name, initial, period, horizon)            \n",
    "    df_p['Model'] = 'Constructed'\n",
    "    df_all = pd.concat([df_all,df_p], axis = 0).reset_index(drop=True)\n",
    "    print(df_all.shape)\n",
    "    \n",
    "    df_all['days'] = df_all['horizon'].astype('timedelta64[D]')\n",
    "    df_all['days'] = df_all['days'].astype(int)\n",
    "\n",
    "    f = plt.figure(figsize=(19, 15))\n",
    "    ax = sns.lineplot(x=\"days\", y=\"smape\", hue=\"Model\", markers=True, data=df_all)\n",
    "    ax.set(xlabel='Date', ylabel='SMAPE')\n",
    "    plt.show();\n",
    "    \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc2e7803-4545-4806-8b82-65d09dc8aa5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def all_mod(train_dset, test_dset, params, df_comp, initial, period, horizon, name):\n",
    "    \n",
    "    train_dset['cap'] = max(int(max(train_dset.y) * 1.1),int(max(test_dset.y) * 1.1))\n",
    "    train_dset['floor'] = 0\n",
    "    test_dset['cap'] = train_dset['cap']\n",
    "    test_dset['floor'] = 0\n",
    "    \n",
    "    y_train = train_dset.y\n",
    "    y_test = test_dset.y\n",
    "    y_test.index = y_test.index + max(y_train.index)\n",
    "    print(y_train.shape[0], y_test.shape[0])\n",
    "    \n",
    "    ### Fit the model using the best parameters ###\n",
    "    \n",
    "    m3_changepoints = (\n",
    "    # 10 potential changepoints in 1 years\n",
    "    pd.date_range('2014-08-01', '2015-06-01', periods=10).date.tolist() +\n",
    "    # 15 potential changepoints in 1 year 2 months\n",
    "    pd.date_range('2015-08-01', '2016-01-01', periods=10).date.tolist()\n",
    "    )\n",
    "\n",
    "    auto_model = Prophet(changepoint_prior_scale = params['changepoint_prior_scale'], \n",
    "                         seasonality_prior_scale = params['seasonality_prior_scale'], \n",
    "                         seasonality_mode = params['seasonality_mode'],\n",
    "                         growth = params['growth'],\n",
    "                         changepoints = m3_changepoints,\n",
    "                         holidays = holidays)\n",
    "    auto_model.add_regressor(train_dset.filter(regex='dcoilwtico').columns[0])\n",
    "    auto_model.add_regressor(train_dset.filter(regex='is_holiday').columns[0])\n",
    "    auto_model.add_regressor(train_dset.filter(regex='onpromotion').columns[0])\n",
    "    auto_model.add_regressor('National')\n",
    "    auto_model.add_regressor('Regional')\n",
    "    auto_model.add_regressor('Local')    \n",
    "    auto_model.add_regressor('Is_month_end')\n",
    "    auto_model.add_regressor('Year')\n",
    "    auto_model.add_regressor('Month')\n",
    "    auto_model.add_regressor('Week')\n",
    "    auto_model.add_regressor('Dayofweek')\n",
    "    auto_model.add_regressor('Dayofyear')\n",
    "    auto_model.add_regressor('Is_month_start')\n",
    "    auto_model.add_regressor('Is_quarter_end')\n",
    "    auto_model.add_regressor('Is_quarter_start')\n",
    "    auto_model.add_regressor('Is_year_end')\n",
    "    auto_model.add_regressor('Is_year_start')\n",
    "\n",
    "    ### Fit the model on the training dataset ###\n",
    "\n",
    "    auto_model.fit(train_dset)\n",
    "\n",
    "    ### Cross validation ###\n",
    "    auto_model_cv, auto_model_p, forecast_auto, prop_test_auto = cv_exam(auto_model, test_dset, y_test, initial, period, horizon)\n",
    "    \n",
    "    fig = auto_model.plot(forecast_auto)\n",
    "    a = add_changepoints_to_plot(fig.gca(), auto_model, forecast_auto)\n",
    "    \n",
    "    mape_ph_auto = mean_absolute_percentage_error(prop_test_auto['y'], prop_test_auto['yhat'], symmetric=True)\n",
    "    \n",
    "    print(f\"The Smape loss value for {name} is {mape_ph_auto:.6f}\")\n",
    "    # 0.111345\n",
    "    print(f\"The RMSE value is {mean_squared_error(prop_test_auto['y'], prop_test_auto['yhat'], squared=False)}\")\n",
    "    # 1186.9104787999713\n",
    "    plot_series(prop_test_auto['yhat'], y_test, labels=[\"y_pred\", \"y_test\"], title = f'{name} Hyperparameter Train-test plot');\n",
    "    \n",
    "    f = plt.figure(figsize=(19, 15))\n",
    "    ax = sns.lineplot(x=\"ds\", y=\"residual\", markers=True, data=auto_model_cv)\n",
    "    ax.set(xlabel='Dates', ylabel='Residuals')\n",
    "    plt.show()\n",
    "\n",
    "    ### Model performance metrics ###\n",
    "    auto_model_pm = performance_metrics(auto_model_cv, rolling_window=1)\n",
    "    print(auto_model_pm)\n",
    "\n",
    "    auto_model_p['Model'] = 'Hyperparameter'\n",
    "\n",
    "    df_comp = pd.concat([df_comp,auto_model_p], axis = 0).reset_index(drop=True)\n",
    "    df_comp['days'] = df_comp['horizon'].astype('timedelta64[D]')\n",
    "    df_comp['days'] = df_comp['days'].astype(int)\n",
    "\n",
    "    f = plt.figure(figsize=(19, 15))\n",
    "    ax = sns.lineplot(x=\"days\", y=\"smape\", hue=\"Model\", markers=True, data=df_comp)\n",
    "    ax.set(xlabel='Date', ylabel='SMAPE')\n",
    "    plt.show();\n",
    "    return df_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d06d6fb-3330-4ffc-bb25-ac9305977243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def hyper(train_dset, test_dset, initial, period, horizon):\n",
    "        \n",
    "    train_dset['cap'] = max(int(max(train_dset.y) * 1.1),int(max(test_dset.y) * 1.1))\n",
    "    train_dset['floor'] = 0\n",
    "    test_dset['cap'] = train_dset['cap']\n",
    "    test_dset['floor'] = 0\n",
    "    \n",
    "    ### Create a list to store MAPE values for each combination ###\n",
    "    mapes = [] \n",
    "\n",
    "    ### Use cross validation to evaluate all parameters ###\n",
    "\n",
    "    for params in all_params:\n",
    "        ### Fit a model using one parameter combination ###\n",
    "        m = Prophet(**params)\n",
    "        m.add_regressor(train_dset.filter(regex='dcoilwtico').columns[0])\n",
    "        m.add_regressor(train_dset.filter(regex='is_holiday').columns[0])\n",
    "        m.add_regressor(train_dset.filter(regex='onpromotion').columns[0])\n",
    "        \n",
    "        m.add_regressor('National')\n",
    "        m.add_regressor('Regional')\n",
    "        m.add_regressor('Local')    \n",
    "        m.add_regressor('Is_month_end')\n",
    "        m.add_regressor('Year')\n",
    "        m.add_regressor('Month')\n",
    "        m.add_regressor('Week')\n",
    "        m.add_regressor('Dayofweek')\n",
    "        m.add_regressor('Dayofyear')\n",
    "        m.add_regressor('Is_month_start')\n",
    "        m.add_regressor('Is_quarter_end')\n",
    "        m.add_regressor('Is_quarter_start')\n",
    "        m.add_regressor('Is_year_end')\n",
    "        m.add_regressor('Is_year_start')\n",
    "        m.fit(train_dset)\n",
    "    \n",
    "        ### Cross-validation ###\n",
    "        df_cv = cross_validation(m, initial = initial, period = period, horizon = horizon, parallel=\"processes\")\n",
    "    \n",
    "        ### Model performance ###\n",
    "        df_p = performance_metrics(df_cv, rolling_window=1)\n",
    "    \n",
    "        ### Save model performance metrics ###\n",
    "        mapes.append(df_p['smape'].values[0])\n",
    "    \n",
    "    ### Tuning results\n",
    "    tuning_results = pd.DataFrame(all_params)\n",
    "    tuning_results['mape'] = mapes\n",
    "    # Find the best parameters\n",
    "    best_params = all_params[np.argmin(mapes)]\n",
    "    print(best_params)\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a5401fa-19ae-4856-818b-5dea3a3eb318",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d0c005f-5db7-440a-9feb-6470ae3b1998",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.key.ifsandboxstorage.dfs.core.windows.net\", dbutils.secrets.get(scope=\"if-databricks-scope\", key=\"if-storage-key\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "670e82ab-356a-4c55-9c2a-4c094177fbf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"abfss://raw@ifsandboxstorage.dfs.core.windows.net/\" + \"/experimental dataset/store-sales/automotive.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").options(inferSchema=\"True\",delimiter=',').csv(path)\n",
    "automotive = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b4f493b-444a-434e-a52c-c6d7a70aeac1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"abfss://raw@ifsandboxstorage.dfs.core.windows.net/\" + \"/experimental dataset/store-sales/babycare.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").options(inferSchema=\"True\",delimiter=',').csv(path)\n",
    "babycare = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a743440-b899-4f8f-a481-9c3e881e3ebf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(babycare.shape)\n",
    "babycare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4b5a087-add4-4c50-a2cc-357c62f118c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"abfss://raw@ifsandboxstorage.dfs.core.windows.net/\" + \"/experimental dataset/store-sales/beauty.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").options(inferSchema=\"True\",delimiter=',').csv(path)\n",
    "beauty = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ba3d504-ac6b-441f-a93c-1dc846486599",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"abfss://raw@ifsandboxstorage.dfs.core.windows.net/\" + \"/experimental dataset/store-sales/beverages.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").options(inferSchema=\"True\",delimiter=',').csv(path)\n",
    "beverages = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8df3136c-81ce-453a-87d7-c31e11a9f75b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"abfss://raw@ifsandboxstorage.dfs.core.windows.net/\" + \"/experimental dataset/store-sales/books.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").options(inferSchema=\"True\",delimiter=',').csv(path)\n",
    "books = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69023d5b-4b3c-43f5-bb55-6a5f671ae144",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Reduce the data down to non leading zeros ###\n",
    "\n",
    "books.Date = books.Date.astype('datetime64[ns]')\n",
    "books = books[books.Date > '2016-10-07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39f294e5-19fd-4fc4-9baa-0efd613905a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "automotive['family'] = 'Automotive'\n",
    "automotive.rename(columns = {'Date':'date'}, inplace = True)\n",
    "babycare['family'] = 'Babycare'\n",
    "babycare.rename(columns = {'Date':'date'}, inplace = True)\n",
    "beauty['family'] = 'Beauty'\n",
    "beauty.rename(columns = {'Date':'date'}, inplace = True)\n",
    "beverages['family'] = 'Beverages'\n",
    "beverages.rename(columns = {'Date':'date'}, inplace = True)\n",
    "books['family'] = 'Books'\n",
    "books.rename(columns = {'Date':'date'}, inplace = True)\n",
    "\n",
    "train = pd.concat([automotive[['date','sales','family']], babycare[['date','sales','family']]], axis = 0)\n",
    "train = pd.concat([train, beauty[['date','sales','family']]], axis = 0)\n",
    "train = pd.concat([train, beverages[['date','sales','family']]], axis = 0)\n",
    "train = pd.concat([train, books[['date','sales','family']]], axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "701665f8-a6f1-47c9-827e-3cc5119dd875",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file = '/dbfs/mnt/experimental dataset/store-sales/store_sales_test_by_product.pickle'\n",
    "\n",
    "with open(file, 'rb') as f:\n",
    "    file_bytes = f.read()\n",
    "    \n",
    "data = pickle.loads(file_bytes)\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "for name in ['AUTOMOTIVE','BABY CARE','BEAUTY','BEVERAGES','BOOKS']:\n",
    "    dset = data[name]\n",
    "    test_df = pd.concat([test_df,dset], axis = 0)    \n",
    "    print(test_df.shape)\n",
    "    \n",
    "test_df.rename(columns = {'Date':'date'}, inplace = True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ae8198-636e-4df2-bd66-74737c5a29b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = train[~train.sales.isna()]\n",
    "print(min(train.date),max(train.date))\n",
    "# 2013-01-01 00:00:00 2017-07-06 00:00:00\n",
    "test_df = test_df[~test_df.date.isna()]\n",
    "print(min(test_df.date),max(test_df.date))\n",
    "# 2017-06-21 00:00:00 2017-08-15 00:00:00\n",
    "test = train[train.date >= min(test_df.date)]\n",
    "print(test.shape)\n",
    "train = train[train.date < min(test_df.date)]\n",
    "print(train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d46ef13-efa0-4625-90aa-66db44022f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"experimental dataset/store-sales/holidays_events.csv\"\n",
    "csvFile = \"abfss://raw@ifsandboxstorage.dfs.core.windows.net/\" + file_location\n",
    "df_raw = spark.read.option(\"header\", \"true\").options(inferSchema='True',delimiter=',').csv(csvFile)\n",
    "holidays_events = df_raw.toPandas()\n",
    "holidays_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c448dd8-3edd-4771-893c-dba1859d3211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(holidays_events.shape)\n",
    "holidays_events.date = pd.to_datetime(holidays_events.date).dt.date\n",
    "holidays_events.date = holidays_events.date.astype('datetime64[ns]')\n",
    "holidays_events_nd = holidays_events[['date']].drop_duplicates()\n",
    "print(holidays_events_nd.shape)\n",
    "\n",
    "for name in list(holidays_events.columns):    \n",
    "    print(f\" There are {holidays_events[name].nunique()} unique {name} ids\")\n",
    "    if holidays_events[name].nunique() < 25:\n",
    "        print(list(set(holidays_events[name])))\n",
    "        \n",
    "for name in list(set(holidays_events['type'])):\n",
    "    print(name,holidays_events[holidays_events.type == str(name)].shape[0])        \n",
    "        \n",
    "holidays_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62502716-0519-4fcc-bfee-eec3988acd10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hol_list = holidays_events[holidays_events.type == 'Holiday']\n",
    "hol_list = list(set(hol_list.date))\n",
    "holi_list = [d.strftime('%d-%m-%Y') for d in hol_list]\n",
    "\n",
    "print(holidays_events.shape)\n",
    "holidays_events = holidays_events[(holidays_events.type != 'Work Day')  & (holidays_events.transferred != True)]\n",
    "holidays_events['var'] = 1\n",
    "print(holidays_events.shape)\n",
    "\n",
    "national = holiday('National')\n",
    "regional = holiday('Regional')\n",
    "local = holiday('Local')\n",
    "print(national.shape,regional.shape,local.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75f21856-7d3e-4b3b-99c3-7191b6a17c5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### Remerge data ###\n",
    "####################\n",
    "\n",
    "train.date = pd.to_datetime(train.date).dt.date\n",
    "train.date = train.date.astype('datetime64[ns]')\n",
    "print(train.shape)\n",
    "\n",
    "add_datepart(train, 'date', drop = False)\n",
    "train['month_date'] = pd.to_datetime(train[['Year', 'Month']].assign(DAY=1))\n",
    "\n",
    "train = pd.merge(train, national,on = 'date',how = 'left')\n",
    "print(train.shape)\n",
    "train = pd.merge(train,regional,on = 'date',how = 'left')\n",
    "print(train.shape)\n",
    "train = pd.merge(train,local,on = 'date',how = 'left')\n",
    "print(train.shape)\n",
    "for name in ['National','Regional','Local']:\n",
    "    train[name] = train[name].fillna(0)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae2c262e-c716-482a-b3c9-42a2146cfa9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### Remerge data ###\n",
    "####################\n",
    "\n",
    "test.date = pd.to_datetime(test.date).dt.date\n",
    "test.date = test.date.astype('datetime64[ns]')\n",
    "print(test.shape)\n",
    "\n",
    "add_datepart(test, 'date', drop = False)\n",
    "test['month_date'] = pd.to_datetime(test[['Year', 'Month']].assign(DAY=1))\n",
    "\n",
    "test = pd.merge(test, national,on = 'date',how = 'left')\n",
    "print(test.shape)\n",
    "test = pd.merge(test,regional,on = 'date',how = 'left')\n",
    "print(test.shape)\n",
    "test = pd.merge(test,local,on = 'date',how = 'left')\n",
    "print(test.shape)\n",
    "for name in ['National','Regional','Local']:\n",
    "    test[name] = test[name].fillna(0)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8180bded-1952-4e42-ad0f-13f0881eae35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Exploratory plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a13b1158-a3e4-4a64-915a-ec50d3f1e865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### Sales over time ###\n",
    "#######################\n",
    "\n",
    "plt.rc('font',size=10)\n",
    "grid = gridspec.GridSpec(3,2)\n",
    "plt.figure(figsize=(19,15))\n",
    "plt.subplots_adjust(wspace=0.4,hspace=0.3)\n",
    "      \n",
    "for idx, name in enumerate(list(set(train.family))):\n",
    "    ax = plt.subplot(grid[idx])\n",
    "    dset = train[train.family == name]\n",
    "\n",
    "    sns.lineplot(x = \"date\", y = \"sales\", markers=True, data=dset)\n",
    "    ax.set_title(f'{name} Sales plot')\n",
    "    ax.set(xlabel='Dates', ylabel='Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03f6f163-a027-4fbe-bf53-329a535c1ec6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "### Percentage change ###\n",
    "#########################\n",
    "\n",
    "plt.rc('font',size=10)\n",
    "grid = gridspec.GridSpec(3,2)\n",
    "plt.figure(figsize=(19,15))\n",
    "plt.subplots_adjust(wspace=0.4,hspace=0.3)\n",
    "      \n",
    "for idx, name in enumerate(list(set(train.family))):\n",
    "    ax = plt.subplot(grid[idx])\n",
    "    dset = train[train.family == name]\n",
    "    dset['Change'] = dset.sales.div(dset.sales.shift())\n",
    "\n",
    "    sns.lineplot(x = \"Dayofyear\", y = \"Change\", hue = 'Year', markers = True, data = dset)\n",
    "    ax.set_title(f'{name} Percentage change plot')\n",
    "    ax.set(xlabel='Dates', ylabel='Sales count percentage change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aab34df5-5e6d-4871-bcd2-843a66cd90f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font',size=10)\n",
    "grid = gridspec.GridSpec(3,2)\n",
    "plt.figure(figsize=(19,15))\n",
    "plt.subplots_adjust(wspace=0.4,hspace=0.3)\n",
    "      \n",
    "for idx, name in enumerate(list(set(train.family))):\n",
    "    ax = plt.subplot(grid[idx])\n",
    "    dset = train[train.family == name]\n",
    "    dset['sales'] = dset['sales'] + 0.01\n",
    "    print(name)\n",
    "    if name != 'Books':\n",
    "        result = seasonal_decompose(dset['sales'], model='multiplicative', period=365)\n",
    "    else:\n",
    "        result = seasonal_decompose(dset['sales'], model='multiplicative', period=31)\n",
    "    ax.set_title(f'{name} Seasonal plot')\n",
    "    result.seasonal.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23b29744-3fe1-40ee-bad9-b61676d23d70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font',size=10)\n",
    "grid = gridspec.GridSpec(3,2)\n",
    "plt.figure(figsize=(19,15))\n",
    "plt.subplots_adjust(wspace=0.4,hspace=0.3)\n",
    "      \n",
    "for idx, name in enumerate(list(set(train.family))):\n",
    "    ax = plt.subplot(grid[idx])\n",
    "    dset = train[train.family == name]\n",
    "    dset['sales'] = dset['sales'] + 0.01\n",
    "\n",
    "    if name != 'Books':\n",
    "        result = seasonal_decompose(dset['sales'], model='multiplicative', period=365)\n",
    "    else:\n",
    "        result = seasonal_decompose(dset['sales'], model='multiplicative', period=31)\n",
    "    ax.set_title(f'{name} Trend plot')\n",
    "    result.trend.plot()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e7d9bd-5452-4919-b223-31c57d87d06c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font',size=10)\n",
    "grid = gridspec.GridSpec(3,2)\n",
    "plt.figure(figsize=(19,15))\n",
    "plt.subplots_adjust(wspace=0.4,hspace=0.3)\n",
    "      \n",
    "for idx, name in enumerate(list(set(train.family))):\n",
    "    ax = plt.subplot(grid[idx])\n",
    "    dset = train[train.family == name]\n",
    "    dset['sales'] = dset['sales'] + 0.01\n",
    "    if name != 'Books':\n",
    "        result = seasonal_decompose(dset['sales'], model='multiplicative', period=365)\n",
    "    else:\n",
    "        result = seasonal_decompose(dset['sales'], model='multiplicative', period=31)\n",
    "    result.plot();\n",
    "    ax.set_title(f'{name} Decomposition plot');\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a01513e7-95a5-48a8-9397-754cc024bd8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "### Align data with Prophet format ###\n",
    "######################################\n",
    " \n",
    "babycare_train = train_dset(babycare,'Babycare',train_df)\n",
    "beauty_train = train_dset(beauty,'Beauty',train_df)\n",
    "automotive_train = train_dset(automotive,'Automotive',train_df)\n",
    "beverages_train = train_dset(beverages,'Beverages',train_df)\n",
    "books_train = train_dset(books,'Books',train_df)\n",
    "\n",
    "babycare_test = train_dset(babycare,'Babycare',test)\n",
    "beauty_test = train_dset(beauty,'Beauty',test)\n",
    "automotive_test = train_dset(automotive,'Automotive',test)\n",
    "beverages_test = train_dset(beverages,'Beverages',test)\n",
    "books_test = train_dset(books,'Books',test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6faa089e-dc49-4122-b126-e86a10445125",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_test_plot(babycare_train,babycare_test,'Babycare')    \n",
    "train_test_plot(beauty_train,beauty_test,'Beauty')    \n",
    "train_test_plot(automotive_train,automotive_test,'Automotive')    \n",
    "train_test_plot(beverages_train,beverages_test,'Beverages')    \n",
    "train_test_plot(books_train,books_test,'Books')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62fb21e5-c705-48c2-8f7b-29c633b95a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hdate = holidays_events.date.dt.date\n",
    "a = list(set(train.month_date.dt.date))\n",
    "a.sort()\n",
    "\n",
    "dates = [x.replace(day = 15) for x in a]\n",
    "\n",
    "holiday = pd.DataFrame({\n",
    "  'holiday': 'holiday',\n",
    "  'ds': pd.to_datetime(list(set(holi_list))),\n",
    "  'lower_window': -1,\n",
    "  'upper_window': 1,\n",
    "})\n",
    "pay_day = pd.DataFrame({\n",
    "  'holiday': 'pay_day',\n",
    "  'ds': pd.to_datetime(dates),\n",
    "  'lower_window': -1,\n",
    "  'upper_window': 1,\n",
    "})\n",
    "month_start = pd.DataFrame({\n",
    "  'holiday': 'month_start',\n",
    "  'ds': pd.to_datetime(a),\n",
    "  'lower_window': -1,\n",
    "  'upper_window': 1,\n",
    "})\n",
    "holidays = pd.concat((holiday, pay_day, month_start))\n",
    "holidays = holidays.drop(['holiday'],axis = 1)\n",
    "print(holidays.shape)\n",
    "holidays = holidays.drop_duplicates()\n",
    "print(holidays.shape)\n",
    "\n",
    "for jan_date in ['2016-01-01','2017-01-01']:\n",
    "    holidays.loc[holidays.ds == pd.to_datetime(jan_date),'lower_window'] = -4\n",
    "    holidays.loc[holidays.ds == pd.to_datetime(jan_date),'upper_window'] = 4\n",
    "holidays['holiday'] = 'Holiday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab217155-eb82-4783-bb4a-51c792931560",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('cmdstanpy')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdcd20f3-33dd-40a5-bb05-4658d5a82e9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "### Models by dataset ###\n",
    "#########################\n",
    "\n",
    "babycare_res = cap_floor(babycare_train, babycare_test, 'Babycare', initial = '730 days', period = '30 days', horizon =  '60 days', growth = 'logistic')    \n",
    "beauty_res = cap_floor(beauty_train, beauty_test, 'Beauty', initial = '730 days', period = '30 days', horizon =  '60 days', growth = 'logistic')    \n",
    "automotive_res = cap_floor(automotive_train, automotive_test, 'Automotive', initial = '730 days', period = '30 days', horizon =  '60 days', growth = 'logistic')    \n",
    "beverages_res = cap_floor(beverages_train, beverages_test, 'Beverages', initial = '730 days', period = '30 days', horizon =  '60 days', growth = 'logistic')    \n",
    "books_res = cap_floor(books_train, books_test, 'Books', initial = '200 days', period = '10 days', horizon =  '20 days', growth = 'logistic')    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a27b4605-7e7d-442d-a9b7-f3a185d15b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###############\n",
    "### Results ###\n",
    "###############\n",
    "        \n",
    "res_dset = pd.DataFrame()\n",
    "\n",
    "res_dset = res_data(babycare_res,'smape','Babycare',res_dset)        \n",
    "res_dset = res_data(automotive_res,'smape','Automotive',res_dset)    \n",
    "res_dset = res_data(beauty_res,'smape','Beauty',res_dset)    \n",
    "res_dset = res_data(beverages_res,'smape','Beverages',res_dset)   \n",
    "res_dset = res_data(books_res,'smape','Books',res_dset)   \n",
    "\n",
    "res_dset = res_data(babycare_res,'rmse','Babycare',res_dset)        \n",
    "res_dset = res_data(automotive_res,'rmse','Automotive',res_dset)    \n",
    "res_dset = res_data(beauty_res,'rmse','Beauty',res_dset)    \n",
    "res_dset = res_data(beverages_res,'rmse','Beverages',res_dset)    \n",
    "res_dset = res_data(books_res,'rmse','Books',res_dset)    \n",
    "\n",
    "res_dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1b8f21f-1966-45a2-9f52-c2bdcee863e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hyperparameter tuning on final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90d9c288-e7da-4246-b459-02b3a3d63dae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holidays_df = pd.DataFrame(holidays[['ds','holiday']])\n",
    "holidays_df.head()\n",
    "\n",
    "m3_changepoints = (\n",
    "    # 10 potential changepoints in 1 years\n",
    "    pd.date_range('2014-08-01', '2015-06-01', periods=10).date.tolist() +\n",
    "    # 15 potential changepoints in 1 year 2 months\n",
    "    pd.date_range('2015-08-01', '2016-01-01', periods=10).date.tolist()\n",
    "    )\n",
    "\n",
    "\n",
    "### Set up parameter grid ###\n",
    "\n",
    "param_grid = {  \n",
    "    'changepoint_prior_scale': [0.001, 0.05, 0.08, 0.5],\n",
    "    'seasonality_prior_scale': [0.01, 1, 5, 10, 12],\n",
    "    'seasonality_mode': ['multiplicative','additive'],\n",
    "    'growth': ['linear','logistic'],\n",
    "    'changepoints' : [m3_changepoints],\n",
    "    'weekly_seasonality': [True],\n",
    "    'yearly_seasonality': [True],\n",
    "    'daily_seasonality': [False],\n",
    "    'holidays': [holidays_df]\n",
    "}\n",
    "\n",
    "### Generate all combinations of parameters ###\n",
    "\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "\n",
    "\n",
    "#beauty_params = hyper(beauty_train, beauty_test, '730 days', '15 days', '30 days')    \n",
    "babycare_params = hyper(babycare_train,babycare_test, '730 days', '15 days', '30 days')    \n",
    "#books_params = hyper(books_train,books_test, '200 days', '5 days', '10 days')    \n",
    "#automotive_params = hyper(automotive_train, automotive_test, '730 days', '15 days', '30 days')    \n",
    "#beverages_params = hyper(beverages_train, beverages_test, '730 days', '15 days', '30 days')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bec3c80-295a-48ec-a523-5b0533033d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('Beauty parameters ', beauty_params)            # linear\n",
    "print('Babycare parameters ', babycare_params)        # linear\n",
    "print('Books parameters ', books_params)              # Logistic\n",
    "print('Automotive parameters ', automotive_params)    # Logistic\n",
    "print('Beverages parameters ', beverages_params)       # linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc608736-0c47-478d-b55f-2e3b31f3a1ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#beauty_all = all_mod(beauty_train,beauty_test, beauty_params, beauty_res, '730 days', '15 days', '30 days', 'Beauty')    \n",
    "babycare_all = all_mod(babycare_train, babycare_test, babycare_params, babycare_res, '730 days', '15 days', '30 days', 'Babycare')    \n",
    "#books_all = all_mod(books_train, books_test, books_params, books_res, '200 days', '5 days', '10 days', 'Books')    \n",
    "#automotive_all = all_mod(automotive_train, automotive_test, automotive_params, automotive_res, '730 days', '15 days', '30 days', 'Automotive')    \n",
    "#beverages_all = all_mod(beverages_train, beverages_test, beverages_params, beverages_res, '730 days', '15 days', '30 days', 'Beverages')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfa03adb-16b3-4694-9622-5a560dca0204",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#perf_vals(beauty_all, meas = 'smape')\n",
    "#perf_vals(babycare_all, meas = 'smape')\n",
    "#perf_vals(beauty_all, meas = 'smape')\n",
    "#perf_vals(automotive_all, meas = 'smape')\n",
    "perf_vals(beverages_all, meas = 'smape')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Prophet_run",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
