{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93039f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modeling\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import accuracy_score to check performance\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d03e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "amz_review = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', names=['review', 'label'])\n",
    "\n",
    "# Take a look at the data\n",
    "amz_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc3102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  1000 non-null   object\n",
      " 1   label   1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset information\n",
    "amz_review.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fefcdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the label distribution\n",
    "amz_review['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25513c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 800 records.\n",
      "The testing dataset has 200 records.\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(amz_review['review'], \n",
    "                                                    amz_review['label'], \n",
    "                                                    test_size = 0.20, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Check the number of records in training and testing dataset.\n",
    "print(f'The training dataset has {len(X_train)} records.')\n",
    "print(f'The testing dataset has {len(X_test)} records.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2689160",
   "metadata": {},
   "source": [
    "* AutoTokenizer.from_pretrained(\"bert-base-cased\") downloads vocabulary from the pretrained bert-base-cased model.  \n",
    "* return_tensors=\"np\" indicates that the return format is NumPy array. Besides np, return_tensors can take the value of tf or pt, where tf returns TensorFlow tf.constant object and pt returns PyTorch torch.tensor object. If not set, it returns a list of python integers.  \n",
    "* padding means adding zeros to shorter reviews in the dataset. The padding argument controls how padding is implemented.  \n",
    "* padding=True is the same as padding='longest'. It checks the longest sequence in the batch and pads zeros to that length. There is no padding if only one text document is provided.  \n",
    "* padding='max_length' pads to max_length if it is specified, otherwise, it pads to the maximum acceptable input length for the model.  \n",
    "* padding=False is the same as padding='do_not_pad'. It is the default, indicating that no padding is applied, so it can output a batch with sequences of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b5caedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 24.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 1.50MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 1.02MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 1.37MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101 17554   112   189  2080  2965   119   102     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer from a pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenize the reviews\n",
    "tokenized_data_train = tokenizer(X_train.to_list(), return_tensors=\"np\", padding=True)\n",
    "tokenized_data_test = tokenizer(X_test.to_list(), return_tensors=\"np\", padding=True)\n",
    "\n",
    "# Labels are one-dimensional numpy or tensorflow array of integers\n",
    "labels_train = np.array(y_train)  \n",
    "labels_test = np.array(y_test) \n",
    "\n",
    "# Tokenized ids\n",
    "print(tokenized_data_train[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e42e3",
   "metadata": {},
   "source": [
    "* TFAutoModelForSequenceClassification loads the BERT model without the sequence classification head.  \n",
    "* The method from_pretrained() loads the weights from the pretrained model into the new model, so the weights in the new model are not randomly initialized. Note that the new weights for the new sequence classification head are going to be randomly initialized.  \n",
    "* bert-base-cased is the name of the pretrained model. We can change it to a different model based on the nature of the project.  \n",
    "* num_labels indicates the number of classes. Our dataset has two classes, positive and negative, so num_labels=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a17a156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tf_model.h5: 100%|██████████| 527M/527M [00:44<00:00, 11.8MB/s] \n",
      "2023-06-05 13:54:09.228688: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf559b",
   "metadata": {},
   "source": [
    "* SparseCategoricalCrossentropy is used as the loss function, but the Hugging Face documentation mentioned that Hugging Face models automatically choose a loss that is appropriate for their task and model architecture if the loss is not explicitly specified.  \n",
    "* from_logits=True informs the loss function that the output values are logits before applying softmax, so the values do not represent probabilities.  \n",
    "* We are using Adam as the optimizer and the number 5e-6 is the learning rate. A smaller learning rate corresponds to a more stable weights value update and a slower training process.  \n",
    "* accuracy is used as the metrics because we have a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed42e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(5e-6), loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8eff9",
   "metadata": {},
   "source": [
    "* batch_size=4 means that four reviews are processed for each weights and bias update.  \n",
    "* epochs=2 means that the model fitting process will go through the training dataset 2 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efe98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "194/200 [============================>.] - ETA: 14s - loss: 0.5941 - accuracy: 0.6920"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(dict(tokenized_data_train), \n",
    "          labels_train, \n",
    "          validation_data=(dict(tokenized_data_test), labels_test),\n",
    "          batch_size=4, \n",
    "          epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481644c",
   "metadata": {},
   "source": [
    "* If we would like to keep the pretrained model weights as is and only update the weights and bias of the output layer, we can use model.layers[0].trainable = False to freeze the weights of the BERT model.  \n",
    "* If we would like to keep the weights of some layers and update others, we can use model.bert.encoder.layer[i].trainable = False to freeze the weights of the corresponding layers.  \n",
    "* In general, if the dataset for the transfer learning model is large, it is suggested to update all weights, and if the dataset for the transfer learning model is small, it is suggested to freeze the pretrained model weights. But we can always compare the model performance by adding the tunable pretrained model layers one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b918b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_test_predict = model.predict(dict(tokenized_data_test))['logits']\n",
    "\n",
    "# First 5 predictions\n",
    "y_test_predict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc57120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities\n",
    "y_test_probabilities = tf.nn.softmax(y_test_predict)\n",
    "\n",
    "# First 5 predicted probabilities\n",
    "y_test_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label\n",
    "y_test_class_preds = np.argmax(y_test_probabilities, axis=1)\n",
    "\n",
    "# First 5 predicted labels\n",
    "y_test_class_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba750bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_score(y_test_class_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73194e",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16daaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./sentiment_transfer_learning_tensorflow/')\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./sentiment_transfer_learning_tensorflow/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4016ece",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad1d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sentiment_transfer_learning_tensorflow/\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = TFAutoModelForSequenceClassification.from_pretrained('./sentiment_transfer_learning_tensorflow/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5187b3",
   "metadata": {},
   "source": [
    "### Sentiment Model Using Transfer Learning on Large Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b199de9",
   "metadata": {},
   "source": [
    "* Firstly, the python dataframe needs to be converted to the Hugging Face arrow dataset using Dataset.from_pandas()  \n",
    "* Then a tokenizer needs to be initiated  \n",
    "* After that, the tokenizer is applied to the Hugging Face arrow dataset  \n",
    "* The pretrained model is loaded using TFAutoModelForSequenceClassification.from_pretrained()  \n",
    "* Finally, the dataset is loaded using prepare_tf_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pyhton dataframe to Hugging Face arrow dataset\n",
    "hg_amz_review = Dataset.from_pandas(amz_review)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Funtion to tokenize data\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data[\"review\"])\n",
    "\n",
    "# Tokenize the dataset\n",
    "dataset = hg_amz_review.map(tokenize_dataset)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# TF dataset\n",
    "tf_dataset = model.prepare_tf_dataset(dataset=dataset, \n",
    "                                      batch_size=16, \n",
    "                                      shuffle=True, \n",
    "                                      tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(5e-6), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(tf_dataset, \n",
    "          epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
