{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfdee8d",
   "metadata": {},
   "source": [
    "BERTopic is a topic modeling python library that combines transformer embeddings and clustering model algorithms to identify topics in NLP (Natual Language Processing). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804056ea",
   "metadata": {},
   "source": [
    "#### Documents Embedding: \n",
    "\n",
    "Firstly, we need to get the embeddings for all the documents. Embeddings are the vector representation of the documents.  \n",
    "  \n",
    "1. BERTopic uses the English version of the sentence_transformers by default to get document embeddings.  \n",
    "2. If there are multiple languages in the document, we can use BERTopic(language=”multilingual”) to support the topic modeling of over 50 languages.  \n",
    "3. BERTopic also supports the pre-trained models from other python packages such as hugging face and flair.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2502e2",
   "metadata": {},
   "source": [
    "#### Documents Clustering: \n",
    "\n",
    "After the text documents have been transformed into embeddings, the next step is to run a clustering model on the embedded documents. Because the embedding vectors usually have very high dimensions, dimension reduction techniques are used to reduce the dimensionalities.  \n",
    "  \n",
    "1. The default algorithm for dimension reduction is UMAP (Uniform Manifold Approximation & Projection). Compared with other dimension reduction techniques such as PCA (Principle Component Analysis), UMAP maintains the data’s local and global structure when reducing the dimensionality, which is important for representing the semantics of the text data. BERTopic provides the option of using other dimensionality reduction techniques by changing the umap_model value in the BERTopic method.  \n",
    "2. The default algorithm for clustering is HDBSCAN. HDBSCAN is a density-based clustering model. It identifies the number of clustering automatically, and does not require specifying the number of clusters beforehand like most of the clustering models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d7d3b",
   "metadata": {},
   "source": [
    "#### Topic Representation: \n",
    "\n",
    "After assigning each document in the corpus into a cluster, the next step is to get the topic representation using a class-based TF-IDF called c-TF-IDF. The top words with the highest c-TF-IDF scores are selected to represent each topic.    \n",
    "  \n",
    "1. c-TF-IDF is similar to TF-IDF in that it measures the term importance by term frequencies while taking into account the whole corpus (all the text data for the analysis).  \n",
    "2. c-TF-IDF is different from TF-IDF in that the term frequency level is different. In the regular TF-IDF, TF measures the term frequency in each document. While in the c-TF-IDF, TF measures the term frequency in each cluster, and each cluster includes many documents.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e11138",
   "metadata": {},
   "source": [
    "#### Maximal Marginal Relevance (MMR) (optional): \n",
    "\n",
    "After extracting the most important terms describing each cluster, there is an optional step to optimize the terms using Maximal Marginal Relevance (MMR). Maximal Marginal Relevance (MMR) has two benefits:  \n",
    "  \n",
    "1. The first benefit is to increase the coherence among the terms for the same topic and remove irrelevant terms.  \n",
    "2. The second benefit is to increase the topic representation by removing synonyms and variations of the same words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d8f26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keithlowton/anaconda3/envs/Bert/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/keithlowton/anaconda3/envs/Bert/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/keithlowton/anaconda3/envs/Bert/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/keithlowton/anaconda3/envs/Bert/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2023-06-05 14:46:27.269083: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935d3ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/keithlowton/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/keithlowton/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/keithlowton/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Text preprocessiong\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "# Topic model\n",
    "from bertopic import BERTopic\n",
    "# Dimension reduction\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eacc3d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  So there is no way for me to plug it in here i...\n",
       "1                        Good case, Excellent value.\n",
       "2                             Great for the jawbone.\n",
       "3  Tied to charger for conversations lasting more...\n",
       "4                                  The mic is great."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "amz_review = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', names=['review', 'label'])\n",
    "# Drop te label \n",
    "amz_review = amz_review.drop('label', axis=1);\n",
    "# Take a look at the data\n",
    "amz_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ae3327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  1000 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset information\n",
    "amz_review.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49181f",
   "metadata": {},
   "source": [
    "### Text Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6df20aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 179 default stopwords. They are ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(f'There are {len(stopwords)} default stopwords. They are {stopwords}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dbc2d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>review_without_stopwords</th>\n",
       "      <th>review_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>way plug US unless go converter.</td>\n",
       "      <td>way plug US unless go converter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>Great jawbone.</td>\n",
       "      <td>Great jawbone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>Tied charger conversations lasting 45 minutes....</td>\n",
       "      <td>Tied charger conversation lasting 45 minutes.M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>mic great.</td>\n",
       "      <td>mic great.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  So there is no way for me to plug it in here i...   \n",
       "1                        Good case, Excellent value.   \n",
       "2                             Great for the jawbone.   \n",
       "3  Tied to charger for conversations lasting more...   \n",
       "4                                  The mic is great.   \n",
       "\n",
       "                            review_without_stopwords  \\\n",
       "0                   way plug US unless go converter.   \n",
       "1                        Good case, Excellent value.   \n",
       "2                                     Great jawbone.   \n",
       "3  Tied charger conversations lasting 45 minutes....   \n",
       "4                                         mic great.   \n",
       "\n",
       "                                   review_lemmatized  \n",
       "0                   way plug US unless go converter.  \n",
       "1                        Good case, Excellent value.  \n",
       "2                                     Great jawbone.  \n",
       "3  Tied charger conversation lasting 45 minutes.M...  \n",
       "4                                         mic great.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "amz_review['review_without_stopwords'] = amz_review['review'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in stopwords]))\n",
    "# Lemmatization\n",
    "amz_review['review_lemmatized'] = amz_review['review_without_stopwords'].apply(lambda x: ' '.join([wn.lemmatize(w) for w in x.split() if w not in stopwords]))\n",
    "# Take a look at the data\n",
    "amz_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a3c0e",
   "metadata": {},
   "source": [
    "### Topic Modeling Using BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd2e717",
   "metadata": {},
   "source": [
    "BERTopic model by default produces different results each time because of the stochasticity inherited from UMAP.  \n",
    "  \n",
    "To get reproducible topics, we need to pass a value to the random_state parameter in the UMAP method.  \n",
    "  \n",
    "n_neighbors=15 means that the local neighborhood size for UMAP is 15. This is the parameter that controls the local versus global structure in data.  \n",
    "1. A low value forces UMAP to focus more on local structure, and may lose insights into the big picture.  \n",
    "2. A high value pushes UMAP to look at the broader neighborhood, and may lose details on local structure.  \n",
    "3. The default n_neighbors values for UMAP is 15.  \n",
    "  \n",
    "n_components=5 indicates that the target dimension from UMAP is 5. This is the dimension of data that will be passed into the clustering model.  \n",
    "min_dist controls how tightly UMAP is allowed to pack points together. It's the minimum distance between points in the low dimensional space.  \n",
    "  \n",
    "1. Small values of min_dist result in clumpier embeddings, which is good for clustering. Since our goal of dimension reduction is to build clustering models, we set min_dist to 0.  \n",
    "2. Large values of min_dist prevent UMAP from packing points together and preserves the broad structure of data.  \n",
    "  \n",
    "metric='cosine' indicates that we will use cosine to measure the distance.  \n",
    "random_state sets a random seed to make the UMAP results reproducible.  \n",
    "  \n",
    "After initiating the UMAP model, we pass it to the BERTopic model, set the language to English, and set the calculate_probabilities parameter to True.\n",
    "  \n",
    "Finally, we pass the processed review documents to the topic model and saved the results for topics and topic probabilities.  \n",
    "  \n",
    "* The values in topics represents the topic each document is assigned to.  \n",
    "* The values in probabilities represents the probability of a document belongs to each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=15, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model, language=\"english\", calculate_probabilities=True)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(amz_review['review_lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4f8a0",
   "metadata": {},
   "source": [
    "### Extract Topics From Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66d4dd4",
   "metadata": {},
   "source": [
    "Using the attribute get_topic_info() on the topic model gives us the list of topics. We can see that the output gives us 31 rows in total.\n",
    "  \n",
    "Topic -1 should be ignored. It indicates that the reviews are not assigned to any specific topic. The count for topic -1 is 322, meaning that there are 322 reviews as outliers that do not belong to any topic.  \n",
    "Topic 0 to topic 29 are the 30 topics created for the reviews. It was ordered by the number of reviews in each topic, so topic 0 has the highest number of reviews.  \n",
    "The Name column lists the top terms for each topic. For example, the top 4 terms for Topic 0 are sound, quality, volume, and audio, indicating that it is a topic related to sound quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of topics\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 terms for a topic\n",
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top topic keywords\n",
    "topic_model.visualize_barchart(top_n_topics=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize term rank decrease\n",
    "topic_model.visualize_term_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c869810",
   "metadata": {},
   "source": [
    "### Topic Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66302d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intertopic distance\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize connections between topics using hierachical clustering\n",
    "topic_model.visualize_hierarchy(top_n_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f07958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity using heatmap\n",
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a6868",
   "metadata": {},
   "source": [
    "### Topic Model Predicted Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability distribution\n",
    "topic_model.visualize_distribution(topic_model.probabilities_[0], min_probability=0.015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9853e",
   "metadata": {},
   "source": [
    "### Topic Model In-sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb515621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic predictions\n",
    "topic_prediction = topic_model.topics_[:]\n",
    "# Save the predictions in the dataframe\n",
    "amz_review['topic_prediction'] = topic_prediction\n",
    "# Take a look at the data\n",
    "amz_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996d0fb",
   "metadata": {},
   "source": [
    "### Topic Model Predictions on New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a5cea",
   "metadata": {},
   "source": [
    "Firstly, let’s decide the number of topics to include in the prediction.  \n",
    "  \n",
    "1. If we would like to assign only one topic to the document, then the number of topics should be 1.  \n",
    "2. If we would like to assign multiple topics to the document, then the number of topics should be greater than 1. Here we are getting the top 3 topics that are most relevant to the new review.  \n",
    "* After that, we pass the new review and the number of topics to the find_topics method. This gives us the topic number and the similarity value.  \n",
    "* Finally, the results are printed. The top 3 similar topics for the new review are topic 1, topic 0, and topic 2. The similarities are 0.43, 0.34, and 0.30.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa290992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data for the review\n",
    "new_review = \"I like the new headphone. Its sound quality is great.\"\n",
    "# Find topics\n",
    "num_of_topics = 3\n",
    "similar_topics, similarity = topic_model.find_topics(new_review, top_n=num_of_topics); \n",
    "# Print results\n",
    "print(f'The top {num_of_topics} similar topics are {similar_topics}, and the similarities are {np.round(similarity,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83192fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top keywords for the top similar topics\n",
    "for i in range(num_of_topics):\n",
    "  print(f'The top keywords for topic {similar_topics[i]} are:')\n",
    "  print(topic_model.get_topic(similar_topics[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07adc5a4",
   "metadata": {},
   "source": [
    "### Save and Load Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the topic model\n",
    "topic_model.save(\"amz_review_topic_model\")\t\n",
    "# Load the topic model\n",
    "my_model = BERTopic.load(\"amz_review_topic_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
